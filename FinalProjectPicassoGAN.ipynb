{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a965400",
      "metadata": {
        "id": "3a965400"
      },
      "source": [
        "# Introduction to Deep Learning: Final Project\n",
        "## Introduction\n",
        "During this class, we touched upon many topics in the field of Deep Learning, and learned about neural networks, stochastic gradient descent, and other techniques and approaches to applying Deep Learning to real world problems.\n",
        "\n",
        "The subject that I was - and still am - most excited by was last week's module on Generative Adversarial Networks (GANs). In particular, the ability for a generator and discriminator to work together to \"learn\" the style of source material. For the Final Project, I wanted to apply that knowledge gained more in-depth to another area of art. Both as a means to better understand how to develop GANs, as well as to explore the fun - and often surprising - output of these systems.\n",
        "\n",
        "My Final Project will be to train a GAN on the data of an artist, in this case [Pablo Picasso](https://en.wikipedia.org/wiki/Pablo_Picasso), a painter whose [cubist](https://en.wikipedia.org/wiki/Cubism) style popularized an entirely new approach and way of thinking about art.\n",
        "\n",
        "Specifically, I have always been fond of Picasso's attempts at portraits, which is where I think cubism really shines. Therefore, **the goal of my Final Project will be to train a GAN on a collection of Picasso's art, in order to produce a model which can transform photos of people's faces into a cubist style.**\n",
        "\n",
        "The code for this project can be found at: https://github.com/buffs28349/IntroDeepLearningFinalProject\n",
        "\n",
        "In order to improve training time, and get more experience using GPUs, I would suggest running this notebook on [Google Colab](https://colab.research.google.com/), a platform for running Jupyter Notebooks in your browser, targeted towards scientific computing and collaboration. You can find a link to this notebook on Colab here: https://colab.research.google.com/drive/1WsVF43K9L31k3GEiRUgYJXGRQLUXGdti?usp=sharing\n",
        "\n",
        "A zip file of the collection of Picasso generated GAN portraits can be found at this Google Drive link: https://drive.google.com/file/d/1B-i7GaSIUSmosu11pRRvvD8WBPkdPkqh/view?usp=sharing There are a half-dozen samples contained in the GitHub repository\n",
        "\n",
        "First, let's begin by importing the necessary libaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd090a1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T02:29:49.486239Z",
          "iopub.status.busy": "2022-09-28T02:29:49.485644Z",
          "iopub.status.idle": "2022-09-28T02:30:12.295601Z",
          "shell.execute_reply": "2022-09-28T02:30:12.294345Z",
          "shell.execute_reply.started": "2022-09-28T02:29:49.486125Z"
        },
        "id": "dbd090a1"
      },
      "outputs": [],
      "source": [
        "# In addition to the usual libraries like Tensorflow/Keras\n",
        "# make sure that these are installed as well\n",
        "!pip install tensorflow_addons\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701520d2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:08:37.562221Z",
          "iopub.status.busy": "2022-09-28T03:08:37.561577Z",
          "iopub.status.idle": "2022-09-28T03:08:42.711227Z",
          "shell.execute_reply": "2022-09-28T03:08:42.710423Z",
          "shell.execute_reply.started": "2022-09-28T03:08:37.562129Z"
        },
        "id": "701520d2"
      },
      "outputs": [],
      "source": [
        "# General (EDA, Plotting, etc)\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import PIL\n",
        "import shutil\n",
        "\n",
        "# Tensorflow/Keras for GAN work\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Define some useful constants\n",
        "RAND_SEED = 123\n",
        "EPOCHS = 65 # Will take a while with a CPU, try using a GPU on Google Colab\n",
        "IMAGE_SIZE = [256, 256]\n",
        "OUTPUT_CHANNELS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, since I am using Google Colab for its GPUs, but you may be running this notebook locally, we need to check the environment we are in, in order to be able to locate the data in the next section."
      ],
      "metadata": {
        "id": "k11pOAm9MUiA"
      },
      "id": "k11pOAm9MUiA"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/content/drive\")\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "DATA_PATH = \"/content/drive/My Drive/picasso_gan_final_data\" if IN_COLAB else \"./data\""
      ],
      "metadata": {
        "id": "QXeDts6WMmDH"
      },
      "id": "QXeDts6WMmDH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "20277ca4",
      "metadata": {
        "id": "20277ca4"
      },
      "source": [
        "### Acquiring the Data\n",
        "After digging through Kaggle's open datasets, I found a gem. [This dataset](https://www.kaggle.com/datasets/techiewaynezheng/picassowikiart256x256) contains 256x256 images, just like last week's GAN data, that was helpfully uploaded by a Kaggle user from [WikiArt](https://www.wikiart.org/). Although it is not necessarily best practice, I have added these images into the GitHub repository for my Final Project, so that my peers may more easily follow along; they are located in the `data/` directory.\n",
        "\n",
        "Additionally, I am using a subset of the [CelebA dataset](https://www.kaggle.com/datasets/jessicali9530/celeba-dataset) as the portraits to be used to generate Picasso-style cubist images from. The dataset is huge - we are only using a small fraction of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48bef93",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:09:03.492739Z",
          "iopub.status.busy": "2022-09-28T03:09:03.492393Z",
          "iopub.status.idle": "2022-09-28T03:09:03.577107Z",
          "shell.execute_reply": "2022-09-28T03:09:03.576400Z",
          "shell.execute_reply.started": "2022-09-28T03:09:03.492708Z"
        },
        "id": "f48bef93"
      },
      "outputs": [],
      "source": [
        "PICASSO_FILES = glob.glob(f\"{DATA_PATH}/picasso/*.jpg\")\n",
        "PORTRAIT_FILES = glob.glob(f\"{DATA_PATH}/portrait/*.jpg\")\n",
        "\n",
        "def decode_image(image_filename):\n",
        "  image_string = tf.io.read_file(image_filename)\n",
        "  image_decoded = tf.image.decode_jpeg(image_string, channels = 3)\n",
        "  image = (tf.cast(image_decoded, tf.float32) / 127.5) - 1\n",
        "  return image\n",
        "\n",
        "def load_data(filenames):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "    ds = ds.map(decode_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "picasso_data = load_data(PICASSO_FILES).batch(1)\n",
        "portrait_data = load_data(PORTRAIT_FILES).batch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b988e0",
      "metadata": {
        "id": "d6b988e0"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "We are developing a GAN to generate images from a training set (Picasso's paintings). In the previous **Introduction** section we loaded our dataset into `picasso_data` and `portrait_data`. We can imagine what this data might be, but it is still important as part of exploratory data analysis to be certain and take a look. First, `picasso_data`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9ad3ab",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:09:05.638245Z",
          "iopub.status.busy": "2022-09-28T03:09:05.637974Z",
          "iopub.status.idle": "2022-09-28T03:09:06.418120Z",
          "shell.execute_reply": "2022-09-28T03:09:06.417355Z",
          "shell.execute_reply.started": "2022-09-28T03:09:05.638214Z"
        },
        "id": "9e9ad3ab"
      },
      "outputs": [],
      "source": [
        "picasso_sample_image = next(iter(picasso_data))\n",
        "plt.subplot(111)\n",
        "plt.title('Picasso Sample Image')\n",
        "plt.imshow(picasso_sample_image[0] * 0.5 + 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51e4ed6e",
      "metadata": {
        "id": "51e4ed6e"
      },
      "source": [
        "Picasso was truly ahead of his time! This is a great example of what we expect: *a 256x256 pixel Picasso cubist painting.* Now one of the portraits that we will use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "643783df",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:09:13.737948Z",
          "iopub.status.busy": "2022-09-28T03:09:13.737671Z",
          "iopub.status.idle": "2022-09-28T03:09:14.036320Z",
          "shell.execute_reply": "2022-09-28T03:09:14.035583Z",
          "shell.execute_reply.started": "2022-09-28T03:09:13.737918Z"
        },
        "id": "643783df"
      },
      "outputs": [],
      "source": [
        "portrait_sample_image = next(iter(portrait_data))\n",
        "plt.subplot(111)\n",
        "plt.title('Portrait Sample Image')\n",
        "plt.imshow(portrait_sample_image[0] * 0.5 + 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84751624",
      "metadata": {
        "id": "84751624"
      },
      "source": [
        "A portrait photo of a person - just what we expected! If it isn't already obvious, we will be training on the images in `picasso_data` and utilizing the GAN to generate Picasso-style cubist images from the source photos in `portrait_data`. The portraits, too, are 256x256 pixels.\n",
        "\n",
        "To get a feel for the amount of training data we have, and the number source images we must generate, let's take a look at the size of both `picasso_data` and `portrait_data`, each of which were loaded from raw images into a Tensorflow [Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77754156",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:09:16.333356Z",
          "iopub.status.busy": "2022-09-28T03:09:16.332739Z",
          "iopub.status.idle": "2022-09-28T03:09:35.109573Z",
          "shell.execute_reply": "2022-09-28T03:09:35.108864Z",
          "shell.execute_reply.started": "2022-09-28T03:09:16.333307Z"
        },
        "id": "77754156"
      },
      "outputs": [],
      "source": [
        "num_picasso = len(PICASSO_FILES)\n",
        "num_portrait = len(PORTRAIT_FILES)\n",
        "\n",
        "print(f\"There are {num_picasso} Picasso training images.\")\n",
        "print(f\"There are {num_portrait} portrait target images.\")\n",
        "\n",
        "plt.subplot(111)\n",
        "plt.bar([\"Picasso\", \"Portrait\"], [num_picasso, num_portrait], color = 'green', width = 0.5)\n",
        "plt.xlabel(\"Dataset\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Total Count of Images in Picasso and Portrait Datasets\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708350b7",
      "metadata": {
        "id": "708350b7"
      },
      "source": [
        "I made sure to collect a strong quantity of both Picasso images - thanks to WikiArt - as well as a large number of potential portraits (albeit not as many as the entire original dataset contained).\n",
        "\n",
        "This allows us to both have a sufficient quantity of training data (1169 Picasso paintings) as well as a sufficient quantity of portraits to generate from (also 1169).\n",
        "\n",
        "Now on to model building and training!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e79262f",
      "metadata": {
        "id": "1e79262f"
      },
      "source": [
        "## Analysis (Model Building + Training)\n",
        "We will be utilizing a [Deep Convolutionl Generative Adversarial Network](https://www.tensorflow.org/tutorials/generative/dcgan) (DCGAN) for our model. We will be using this because Tensorflow has strong support for this type of GAN, and I thoroughly enjoed learning about and developing with it during Week 5.\n",
        "\n",
        "I was inspired by the code [from here](https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook). However, I improved on the process myself for a more performant and effective training procedure specific to the requirements for our mini-project this week. ***The work is entirely my own.***\n",
        "\n",
        "### Generator\n",
        "The first part of a GAN is of course the generator. We start by creating helper functions, `downsample` and `upsample` to be used when creating the generator.\n",
        "\n",
        "Our generator consists of first downsampling our Picasso painting images, before upscaling them back to the required resolution (256x256). This CNN (the \"DC\" in DCGAN) process allows us to more thoroughly iterate on the learning procedure, providing many benefits - most notably, a better loss value and therefore enhanced output quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f3b151",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:09:41.824710Z",
          "iopub.status.busy": "2022-09-28T03:09:41.823569Z",
          "iopub.status.idle": "2022-09-28T03:09:41.832359Z",
          "shell.execute_reply": "2022-09-28T03:09:41.831676Z",
          "shell.execute_reply.started": "2022-09-28T03:09:41.824671Z"
        },
        "id": "39f3b151"
      },
      "outputs": [],
      "source": [
        "def downsample(filters, size, apply_instancenorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "        kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "    if apply_instancenorm:\n",
        "        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    result.add(layers.LeakyReLU())\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n",
        "        padding='same',\n",
        "        kernel_initializer=initializer,\n",
        "        use_bias=False))\n",
        "\n",
        "    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n",
        "\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "\n",
        "    result.add(layers.ReLU())\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "049ab3a9",
      "metadata": {
        "id": "049ab3a9"
      },
      "source": [
        "After defining our `downsample` and `upsample` functions we can use them to assemble the layers of our generator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ba4689",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:09:53.987531Z",
          "iopub.status.busy": "2022-09-28T03:09:53.987264Z",
          "iopub.status.idle": "2022-09-28T03:09:55.280163Z",
          "shell.execute_reply": "2022-09-28T03:09:55.279424Z",
          "shell.execute_reply.started": "2022-09-28T03:09:53.987501Z"
        },
        "id": "17ba4689"
      },
      "outputs": [],
      "source": [
        "def make_generator():\n",
        "    inputs = layers.Input(shape=[256,256,3])\n",
        "\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_instancenorm=False),\n",
        "        downsample(128, 4),\n",
        "        downsample(256, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "        downsample(512, 4),\n",
        "    ]\n",
        "\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4),\n",
        "        upsample(256, 4),\n",
        "        upsample(128, 4),\n",
        "        upsample(64, 4),\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "        strides=2,\n",
        "        padding='same',\n",
        "        kernel_initializer=initializer,\n",
        "        activation='tanh')\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # Perform downsampling\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    # Perform upsampling\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=x)\n",
        "    \n",
        "generator = make_generator()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3bf656",
      "metadata": {
        "id": "7e3bf656"
      },
      "source": [
        "Let's view the generator's summary to confirm it produced the layers that we expect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ce326b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:09:58.503438Z",
          "iopub.status.busy": "2022-09-28T03:09:58.502714Z",
          "iopub.status.idle": "2022-09-28T03:09:58.524110Z",
          "shell.execute_reply": "2022-09-28T03:09:58.523203Z",
          "shell.execute_reply.started": "2022-09-28T03:09:58.503401Z"
        },
        "id": "f3ce326b"
      },
      "outputs": [],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f204e49",
      "metadata": {
        "id": "3f204e49"
      },
      "source": [
        "### Discriminator\n",
        "Next is the discriminator. It is similar to the generator but without upsampling. Essentially our layers will continuously refine its understanding of the image before being used to make a decision as to whether the image is a real Picasso or a fake: binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6163eb16",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:10:03.326298Z",
          "iopub.status.busy": "2022-09-28T03:10:03.325730Z",
          "iopub.status.idle": "2022-09-28T03:10:03.499406Z",
          "shell.execute_reply": "2022-09-28T03:10:03.498679Z",
          "shell.execute_reply.started": "2022-09-28T03:10:03.326262Z"
        },
        "id": "6163eb16"
      },
      "outputs": [],
      "source": [
        "def make_discriminator():\n",
        "    initializer = tf.random_normal_initializer(0.0, 0.02)\n",
        "    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "\n",
        "    x = inp\n",
        "\n",
        "    down1 = downsample(64, 4, False)(x)\n",
        "    down2 = downsample(128, 4)(down1)\n",
        "    down3 = downsample(256, 4)(down2)\n",
        "\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3)\n",
        "    conv = layers.Conv2D(512, 4, strides=1,\n",
        "        kernel_initializer=initializer,\n",
        "        use_bias=False)(zero_pad1)\n",
        "\n",
        "    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n",
        "\n",
        "    leaky_relu = layers.LeakyReLU()(norm1)\n",
        "\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
        "\n",
        "    last = layers.Conv2D(1, 4, strides=1,\n",
        "        kernel_initializer=initializer)(zero_pad2)\n",
        "\n",
        "    return tf.keras.Model(inputs=inp, outputs=last)\n",
        "\n",
        "discriminator = make_discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccdaaebd",
      "metadata": {
        "id": "ccdaaebd"
      },
      "source": [
        "And the discriminator's summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6fa0905",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:10:05.936077Z",
          "iopub.status.busy": "2022-09-28T03:10:05.935786Z",
          "iopub.status.idle": "2022-09-28T03:10:05.947980Z",
          "shell.execute_reply": "2022-09-28T03:10:05.947068Z",
          "shell.execute_reply.started": "2022-09-28T03:10:05.936048Z"
        },
        "id": "a6fa0905"
      },
      "outputs": [],
      "source": [
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1d8a187",
      "metadata": {
        "id": "c1d8a187"
      },
      "source": [
        "**Important Note:** Since we have not yet trained the generator, its output will be nothing of value to us. This is demonstrated below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aee54f0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:10:08.341187Z",
          "iopub.status.busy": "2022-09-28T03:10:08.340916Z",
          "iopub.status.idle": "2022-09-28T03:10:15.833847Z",
          "shell.execute_reply": "2022-09-28T03:10:15.832553Z",
          "shell.execute_reply.started": "2022-09-28T03:10:08.341156Z"
        },
        "id": "0aee54f0"
      },
      "outputs": [],
      "source": [
        "to_picasso = generator(portrait_sample_image)\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original\")\n",
        "plt.imshow(portrait_sample_image[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(\"Picasso Cubist\")\n",
        "plt.imshow(to_picasso[0] * 0.5 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4be320e0",
      "metadata": {
        "id": "4be320e0"
      },
      "source": [
        "### Training\n",
        "Now this is where the magic happens. We subclass `keras.Model` and create our own `PicassoGAN` from it. Specifically, we will override the `train_step` to better track the quality of our data during fitting, as well as customize the generator/discriminator relationship of the GAN itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f50aaca2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:10:20.179345Z",
          "iopub.status.busy": "2022-09-28T03:10:20.179010Z",
          "iopub.status.idle": "2022-09-28T03:10:20.189054Z",
          "shell.execute_reply": "2022-09-28T03:10:20.187968Z",
          "shell.execute_reply.started": "2022-09-28T03:10:20.179294Z"
        },
        "id": "f50aaca2"
      },
      "outputs": [],
      "source": [
        "class PicassoGAN(keras.Model):\n",
        "  def __init__(self, generator, discriminator,):\n",
        "    super(PicassoGAN, self).__init__()\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "        \n",
        "  def compile(self, generator_optimizer, discriminator_optimizer, gen_loss_fn, disc_loss_fn):\n",
        "    super(PicassoGAN, self).compile()\n",
        "    self.generator_optimizer = generator_optimizer\n",
        "    self.discriminator_optimizer = discriminator_optimizer\n",
        "    self.gen_loss_fn = gen_loss_fn\n",
        "    self.disc_loss_fn = disc_loss_fn\n",
        "\n",
        "  # Necessary for saving model\n",
        "  # Source: https://stackoverflow.com/a/69460770\n",
        "  def call(self, inputs, training=False):\n",
        "    self.weight_dict['backbone'].trainable = False\n",
        "    x = self.weight_dict['backbone'](inputs)\n",
        "    x = self.weight_dict['outputs'](x)\n",
        "    return x\n",
        "        \n",
        "  def train_step(self, batch_data):\n",
        "    real_picasso, real_portrait = batch_data\n",
        "        \n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Generate fake Picassos from input portraits\n",
        "      fake_picasso = self.generator(real_portrait, training=True)\n",
        "            \n",
        "      # Discriminator\n",
        "      disc_real_picasso = self.discriminator(real_picasso, training=True)\n",
        "      disc_fake_picasso = self.discriminator(fake_picasso, training=True)\n",
        "\n",
        "      # Generator loss\n",
        "      gen_loss = self.gen_loss_fn(disc_fake_picasso)\n",
        "            \n",
        "      # Discriminator loss\n",
        "      disc_loss = self.disc_loss_fn(disc_real_picasso, disc_fake_picasso)\n",
        "\n",
        "    # Calculate gradients\n",
        "    generator_gradients = tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "    discriminator_gradients = tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "        \n",
        "    # Apply gradients to optimizer\n",
        "    self.generator_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))\n",
        "    self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients, self.discriminator.trainable_variables))\n",
        "        \n",
        "    return {\n",
        "      \"gen_loss\": gen_loss,\n",
        "      \"disc_loss\": disc_loss,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a030f35",
      "metadata": {
        "id": "8a030f35"
      },
      "source": [
        "Lastly, we must define our loss functions. These compare the real images and fake images. A \"perfect\" discriminator will produce all correct, or true values (that is `1`s) and a poor generator all incorrect values (`0`s).\n",
        "\n",
        "It has already been said, but is worth repeating: the incredible power of a GAN is in combining these together so that the discriminator improves the generator by identifying real vs fakes, thus requiring the generator to improve itself to \"trick\" the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439f32e1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:10:33.959080Z",
          "iopub.status.busy": "2022-09-28T03:10:33.958488Z",
          "iopub.status.idle": "2022-09-28T03:10:33.966054Z",
          "shell.execute_reply": "2022-09-28T03:10:33.965348Z",
          "shell.execute_reply.started": "2022-09-28T03:10:33.959041Z"
        },
        "id": "439f32e1"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real, generated):\n",
        "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n",
        "    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n",
        "    total_disc_loss = real_loss + generated_loss\n",
        "    return total_disc_loss * 0.5\n",
        "    \n",
        "def generator_loss(generated):\n",
        "    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e426fb86",
      "metadata": {
        "id": "e426fb86"
      },
      "source": [
        "Now we may perform the actual training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f984c6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:10:41.062409Z",
          "iopub.status.busy": "2022-09-28T03:10:41.061679Z",
          "iopub.status.idle": "2022-09-28T03:24:24.096688Z",
          "shell.execute_reply": "2022-09-28T03:24:24.095975Z",
          "shell.execute_reply.started": "2022-09-28T03:10:41.062372Z"
        },
        "id": "17f984c6"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1 = 0.5)\n",
        "\n",
        "picasso_model = PicassoGAN(generator, discriminator)\n",
        "\n",
        "picasso_model.compile(generator_optimizer, discriminator_optimizer, generator_loss, discriminator_loss)\n",
        "\n",
        "picasso_model.fit(tf.data.Dataset.zip((picasso_data, portrait_data)), epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will save our model so that we don't have to retrain it in the future!"
      ],
      "metadata": {
        "id": "tOkJpzu2v2BY"
      },
      "id": "tOkJpzu2v2BY"
    },
    {
      "cell_type": "code",
      "source": [
        "picasso_model.compute_output_shape(input_shape=(None, 256, 256, 3))\n",
        "picasso_model.save(f\"{DATA_PATH}/picasso.model\")"
      ],
      "metadata": {
        "id": "Rbo2mS81v6AR"
      },
      "id": "Rbo2mS81v6AR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "35e5207a",
      "metadata": {
        "id": "35e5207a"
      },
      "source": [
        "## Results\n",
        "We have trained our model! Let's see what it can do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef69c0f9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:24:41.784698Z",
          "iopub.status.busy": "2022-09-28T03:24:41.784375Z",
          "iopub.status.idle": "2022-09-28T03:24:42.462818Z",
          "shell.execute_reply": "2022-09-28T03:24:42.462132Z",
          "shell.execute_reply.started": "2022-09-28T03:24:41.784659Z"
        },
        "id": "ef69c0f9"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(3, 2, figsize=(12, 12))\n",
        "for i, img in enumerate(portrait_data.take(3)):\n",
        "    prediction = generator(img, training=False)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "    ax[i, 0].imshow(img)\n",
        "    ax[i, 1].imshow(prediction)\n",
        "    ax[i, 0].set_title(\"Real Portrait Input\")\n",
        "    ax[i, 1].set_title(\"Fake Picasso Output\")\n",
        "    ax[i, 0].axis(\"off\")\n",
        "    ax[i, 1].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d782d012",
      "metadata": {
        "id": "d782d012"
      },
      "source": [
        "### Save Output to Share\n",
        "We will create a dedicated directory, `cubist_portrait_output` to save each portrait to. Plus, we will archive them into a zip file for easily distribution and sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa3b4d3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-28T03:31:14.239162Z",
          "iopub.status.busy": "2022-09-28T03:31:14.238868Z",
          "iopub.status.idle": "2022-09-28T03:31:25.502464Z",
          "shell.execute_reply": "2022-09-28T03:31:25.501599Z",
          "shell.execute_reply.started": "2022-09-28T03:31:14.239128Z"
        },
        "id": "6aa3b4d3"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(f\"{DATA_PATH}/cubist_portrait_output\"):\n",
        "  os.mkdir(f\"{DATA_PATH}/cubist_portrait_output\")\n",
        "\n",
        "i = 1\n",
        "for img in portrait_data:\n",
        "    prediction = generator(img, training=False)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    im = PIL.Image.fromarray(prediction)\n",
        "    im.save(f\"{DATA_PATH}/cubist_portrait_output/{str(i)}.jpg\")\n",
        "    i += 1\n",
        "    \n",
        "shutil.make_archive(f\"{DATA_PATH}/cubist_portraits\", 'zip', f\"{DATA_PATH}/cubist_portrait_output\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e047815",
      "metadata": {
        "id": "9e047815"
      },
      "source": [
        "## Discussion/Conclusion\n",
        "Despite the generated images from the input portraits not quite looking like Picasso's cubist style, I still consider this Final Project a success. We are able to improve on our understanding of GANs from Week 5 by applying the same EDA and model building methodology to a novel problem (generation of Picasso cubist images). Our loss was acceptable (`< 0.9`), albeit the result did not resemble Picasso.\n",
        "\n",
        "This knowledge is still useful, because it teaches us that refinement of the layers of our generator and discriminator is warranted. Additionally, tweaking hyperparameters is in order, both to better improve model training efficiency as well as improve model output.\n",
        "\n",
        "I learned quite a lot in this class, and am especially fond of my learning of GANs. I hope to further expand my Data Science skillset as applied to Machine Learning and Artificial Intelligence."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}